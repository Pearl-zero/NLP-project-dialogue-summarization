{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back Translation Function\n",
    "def back_translation(text):\n",
    "    \"\"\"\n",
    "    Perform back translation (Korean → English → Korean) for text augmentation.\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translated = translator.translate(text, src='ko', dest='en').text\n",
    "        return translator.translate(translated, src='en', dest='ko').text\n",
    "    except Exception as e:\n",
    "        print(f\"Back translation error: {e}\")\n",
    "        return text  # Fallback to original text if error occurs\n",
    "\n",
    "# Random Deletion\n",
    "def random_deletion(sentence, p=0.1):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:\n",
    "        return sentence\n",
    "    return ' '.join([word for word in words if random.uniform(0, 1) > p])\n",
    "\n",
    "# Random Word Swap\n",
    "def random_swap(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        if len(words) > 1:\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Random Insertion\n",
    "def random_insertion(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        words.insert(idx, words[idx])\n",
    "    return ' '.join(words)\n",
    "\n",
    "# AEDA (Punctuation Insertion)\n",
    "def aeda_augmentation(sentence, num_punct=1):\n",
    "    punctuations = ['.', ',', '!', '?', ';']\n",
    "    words = sentence.split()\n",
    "    for _ in range(num_punct):\n",
    "        idx = random.randint(0, len(words))\n",
    "        punct = random.choice(punctuations)\n",
    "        words.insert(idx, punct)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Augmentation Function\n",
    "def augment_text_data(text_col, repetition=1):\n",
    "    \"\"\"\n",
    "    Apply text augmentations (EDA, AEDA, Back-Translation) for each input text.\n",
    "    \n",
    "    Ensures output length matches input length by selecting only one augmented version per text.\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    for text in tqdm(text_col, desc=\"Augmenting Text\"):\n",
    "        # Store augmented versions\n",
    "        augmented_versions = []\n",
    "        \n",
    "        for _ in range(repetition):\n",
    "            try:\n",
    "                eda_augmented = random_deletion(text, p=0.1)\n",
    "                eda_augmented = random_swap(eda_augmented, n=1)\n",
    "                eda_augmented = random_insertion(eda_augmented, n=1)\n",
    "\n",
    "                aeda_augmented = aeda_augmentation(text, num_punct=1)\n",
    "                back_translated = back_translation(text)\n",
    "\n",
    "                # Collect augmented versions\n",
    "                augmented_versions.extend([eda_augmented, aeda_augmented, back_translated])\n",
    "            except Exception as e:\n",
    "                print(f\"Augmentation error: {e}\")\n",
    "                augmented_versions.append(text)  # Fallback to original text\n",
    "        \n",
    "        # Select one augmented version (e.g., the first) to maintain alignment\n",
    "        augmented_data.append(augmented_versions[0])\n",
    "    \n",
    "    return augmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Person1#: 안녕하세요, Smith. 저는 Hawkins 의사입니다. 오늘 왜 왔습니까?\\n#person2#: 건강 검진을받는 것이 좋은 생각이라고 생각합니다.\\n#person1#: 예, 5 년 동안 건강 수표를받지 못했습니다. 매년 가져와야합니다.\\n#person2#: 알고 있습니다. 그러나 문제가 없다면 왜 의사를 만나러 가야합니까?\\n#person1#: 심각한 질병을 피하는 가장 좋은 방법은 일찍 찾는 것입니다. 따라서 건강을 위해 매년 한 번 이상 오십시오.\\n#person2#: 좋아요.\\n#person1#: 여기. 눈과 귀가 잘 보입니다.\\n#person2#: 예.\\n#person1#: 아시다시피, 담배는 폐암과 심장병의 주요 원인입니다.\\n#Person2#: 수백 번 시도했지만 습관을 버리는 것은 어렵습니다.\\n#person1#: 우리는 도움이 될 수있는 수업과 약물을 제공합니다. 외출하기 전에 더 많은 정보를 제공 할 것입니다.\\n#person2#: 좋아요, 감사합니다, 의사.\",\"Smith 씨는 건강 점검을 받고 있으며 Hawkins는 매년 건강 검진을받을 것을 권장합니다. Hawkins는 Smith 씨가 흡연을 중단 할 수있는 수업 및 약물에 대한 정보를 제공 할 것입니다.\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translation(\"\"\"\n",
    "    #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
    "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
    "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
    "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
    "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
    "#Person2#: 알겠습니다.\n",
    "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
    "#Person2#: 네.\n",
    "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
    "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
    "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
    "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\",\"스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\",건강검진 받기\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. \n",
    "호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\",건강검진 받기,\n",
    "\"#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요? 번 건강검진을 받는 것이 좋을 것 같아서요. \n",
    "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 매년 받아야 합니다. \n",
    "#Person2#: 알고 하지만 아무 문제가 없다면 왜 의사를 가야 하나요? \n",
    "#Person1#: 심각한 질병을 피하는 가장 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 \n",
    "#Person2#: 알겠습니다. #Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 네. \n",
    "#Person1#: 당신도 담배는 폐암과 심장병의 주요 원인입니다. 끊으셔야 합니다. \n",
    "#Person2#: 수백 #Person2#: 시도했지만, 습관을 버리는 것이 어렵습니다. \n",
    "#Person1#: 우리는 도움이 될 수 있는 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다. \n",
    "# #Person2#: 알겠습니다, 감사합니다, 의사선생님.\"\n",
    "train_7354,\"#Person1#: 아빠, 숙제 다 했어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Text: 100%|██████████| 4152/4152 [1:20:35<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Load the data from train.csv\n",
    "train_data = pd.read_csv('/root/NLP/data/train.csv')\n",
    "\n",
    "# Sample a smaller subset of the data (e.g., 1/3 of the original data)\n",
    "subset_size = int(len(train_data) / 3)  # For approximately 2 hours of processing\n",
    "train_sample = train_data.sample(n=subset_size, random_state=42)\n",
    "\n",
    "# Apply augmentation to the sampled 'dialogue' column (optional: reduce repetition if needed)\n",
    "train_sample['augmented'] = augment_text_data(train_sample['dialogue'], repetition=1)\n",
    "\n",
    "# Save the augmented data to a new file\n",
    "train_sample.to_csv(\"/root/NLP/data/aug_train.csv\", index=False)\n",
    "\n",
    "print(\"Data augmentation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Load the data from train.csv\n",
    "train_data = pd.read_csv('/root/NLP/data/train.csv')\n",
    "\n",
    "# Sample a smaller subset of the data (e.g., 1/3 of the original data)\n",
    "subset_size = int(len(train_data) / 3)  # For approximately 2 hours of processing\n",
    "train_sample = train_data.sample(n=subset_size, random_state=42)\n",
    "\n",
    "# Apply augmentation to the sampled 'dialogue' column (optional: reduce repetition if needed)\n",
    "train_sample['dialogue'] = augment_text_data(train_sample['dialogue'], repetition=1)\n",
    "\n",
    "# Save the augmented data to a new file\n",
    "train_sample.to_csv(\"/root/NLP/data/aug_train.csv\", index=False)\n",
    "\n",
    "print(\"Data augmentation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 순서\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation and combination complete!\n"
     ]
    }
   ],
   "source": [
    "# We'll keep the format consistent with fname, dialogue, summary, topic\n",
    "augmented_data = train_sample[['fname', 'dialogue', 'summary', 'topic']]  # Keep the same format\n",
    "\n",
    "# Combine the original train data and the augmented data (including augmented dialogue)\n",
    "combined_data = pd.concat([train_data[['fname', 'dialogue', 'summary', 'topic']], \n",
    "                           augmented_data[['fname', 'dialogue', 'summary', 'topic']]], \n",
    "                          axis=0, ignore_index=True)\n",
    "\n",
    "# Save the combined data (it will include original and augmented data)\n",
    "combined_data.to_csv(\"/root/NLP/data/combined_train.csv\", index=False)\n",
    "\n",
    "print(\"Data augmentation and combination complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4152.000000\n",
      "mean      440.151734\n",
      "std       225.150379\n",
      "min       103.000000\n",
      "25%       297.000000\n",
      "50%       397.000000\n",
      "75%       541.000000\n",
      "max      2339.000000\n",
      "Name: dialogue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_dialog_length = train_sample['dialogue'].apply(lambda x:len(x))\n",
    "\n",
    "print(train_dialog_length.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fname                                           dialogue  \\\n",
      "0  train_0  #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...   \n",
      "1  train_1  #Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...   \n",
      "2  train_2  #Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...   \n",
      "3  train_3  #Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...   \n",
      "4  train_4  #Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...   \n",
      "\n",
      "                                             summary     topic  \n",
      "0  스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...   건강검진 받기  \n",
      "1  파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...        백신  \n",
      "2  #Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...     열쇠 찾기  \n",
      "3  #Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...  여자친구가 있다  \n",
      "4  말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...        댄스  \n",
      "Number of rows (sentences): 4152\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Preview the first few rows to see the data content\n",
    "print(combined_data.head())\n",
    "\n",
    "# Check dataset size\n",
    "num_rows, num_columns = augmented_data.shape\n",
    "print(f\"Number of rows (sentences): {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
