{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. 스페셜 토큰 처리?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할 x 단어\n",
    "# not_tokenized = ['#Address#', '#CarNumber#', '#CardNumber#', '#DateOfBirth#', '#Email#', \n",
    "#                 '#PassportNumber#', '#Person#', '#Person1#', '#Person2#', '#Person3#', \n",
    "#                 '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PhoneNumber#', '#SSN#']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 발화자 / 대화 분리해서 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from koeda import EasyDataAugmentation\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Augmenter 초기화 (Synonym Replacement)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m augmenter \u001b[38;5;241m=\u001b[39m \u001b[43mnaw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSynonymAug\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwordnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 증강 데이터를 저장할 리스트\u001b[39;00m\n\u001b[1;32m      9\u001b[0m augmented_rows \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/augmenter/word/synonym.py:66\u001b[0m, in \u001b[0;36mSynonymAug.__init__\u001b[0;34m(self, aug_src, model_path, name, aug_min, aug_max, aug_p, lang, stopwords, tokenizer, reverse_tokenizer, stopwords_regex, force_reload, verbose)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m lang\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/augmenter/word/synonym.py:163\u001b[0m, in \u001b[0;36mSynonymAug.get_model\u001b[0;34m(cls, aug_src, lang, dict_path, force_reload)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;28mcls\u001b[39m, aug_src, lang, dict_path, force_reload):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m aug_src \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnmw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWordNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_synonym\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m aug_src \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppdb\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m init_ppdb_model(dict_path\u001b[38;5;241m=\u001b[39mdict_path, force_reload\u001b[38;5;241m=\u001b[39mforce_reload)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/model/word_dict/wordnet.py:33\u001b[0m, in \u001b[0;36mWordNet.__init__\u001b[0;34m(self, lang, is_synonym)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissed nltk library. Install nltk by `pip install nltk`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     # Check whether wordnet package is downloaded\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     wordnet.synsets('computer')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     nltk.download('wordnet')\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     nltk.download('averaged_perceptron_tagger')\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/model/word_dict/wordnet.py:37\u001b[0m, in \u001b[0;36mWordNet.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m         \u001b[43mwordnet\u001b[49m\u001b[38;5;241m.\u001b[39msynsets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wordnet\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordnet' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('/root/NLP/data/train.csv')\n",
    "df_filtered = df[['fname', 'dialogue', 'summary', 'topic']]\n",
    "\n",
    "# Augmenter 초기화 (Synonym Replacement)\n",
    "augmenter = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=3)\n",
    "\n",
    "# 증강 데이터를 저장할 리스트\n",
    "augmented_rows = []\n",
    "\n",
    "# 증강 함수 정의\n",
    "def augment_text_data_with_nlpaug(text, repetition=3):\n",
    "    \"\"\"입력된 문장에 대해 nlpaug를 사용하여 데이터 증강\"\"\"\n",
    "    return [augmenter.augment(text) for _ in range(repetition)]\n",
    "\n",
    "# 화자 태그와 텍스트를 분리하는 함수\n",
    "def split_speaker_and_text(dialogue):\n",
    "    \"\"\"화자와 대화 내용을 분리하여 리스트로 반환\"\"\"\n",
    "    lines = dialogue.split('\\n')\n",
    "    speaker_text_pairs = []\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            speaker, text = line.split(':', 1)\n",
    "            speaker_text_pairs.append((speaker.strip(), text.strip()))\n",
    "    return speaker_text_pairs\n",
    "\n",
    "# 화자 태그와 텍스트를 결합하는 함수\n",
    "def join_speaker_and_text(speaker_text_pairs):\n",
    "    \"\"\"화자와 대화 내용을 결합하여 하나의 문자열로 반환\"\"\"\n",
    "    return '\\n'.join([f\"{speaker}: {text}\" for speaker, text in speaker_text_pairs])\n",
    "\n",
    "# 데이터 증강 수행\n",
    "for index, row in df_filtered.iterrows():\n",
    "    original_text = row['dialogue']\n",
    "\n",
    "    # 화자 태그와 대화 내용 분리\n",
    "    speaker_text_pairs = split_speaker_and_text(original_text)\n",
    "\n",
    "    # 대화 내용에 nlpaug 적용\n",
    "    augmented_text_lists = [\n",
    "        augment_text_data_with_nlpaug(text, repetition=3) for _, text in speaker_text_pairs\n",
    "    ]\n",
    "\n",
    "    # nlpaug 결과와 화자 태그 결합\n",
    "    for augmented_texts in zip(*augmented_text_lists):\n",
    "        new_speaker_text_pairs = [\n",
    "            (speaker_text_pairs[i][0], augmented_text)\n",
    "            for i, augmented_text in enumerate(augmented_texts)\n",
    "        ]\n",
    "        augmented_dialogue = join_speaker_and_text(new_speaker_text_pairs)\n",
    "\n",
    "        # 증강 데이터를 새로운 행으로 추가\n",
    "        augmented_rows.append({\n",
    "            'fname': row['fname'],\n",
    "            'dialogue': augmented_dialogue,\n",
    "            'summary': row['summary'],\n",
    "            'topic': row['topic']\n",
    "        })\n",
    "\n",
    "    # 진행 상황 출력\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Processed {index + 1} rows out of {len(df_filtered)}\")\n",
    "\n",
    "# 증강 데이터를 DataFrame으로 변환\n",
    "augmented_data = pd.DataFrame(augmented_rows)\n",
    "\n",
    "# 결과 저장\n",
    "augmented_data.to_csv('/root/NLP/data/aug_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. df + augmented_data concat하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 인덱스 reset / fname 열 바꾸기(train_0 ~ train?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
